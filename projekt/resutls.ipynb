{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as t\n",
    "import pyspark.sql.functions as f\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from pyspark.sql.functions import col, sum, count\n",
    "import dataframe_image as dfi\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "file_path = \"/home/olek/studia/semestr_3_AM/Big_Data/Big_Data/projekt/data/charts.csv\"\n",
    "# file_path = \"/home/olek/studia/semestr_3_AM/Big_Data/Big_Data/projekt/data/sample.csv\"\n",
    "\n",
    "# results_path = \"/home/olek/studia/semestr_3_AM/Big_Data/Big_Data/projekt/sample_results/\"\n",
    "if file_path.endswith(\"sample.csv\"):\n",
    "    results_path = \"/home/olek/studia/semestr_3_AM/Big_Data/Big_Data/projekt/sample_results/\"\n",
    "    images_path = \"/home/olek/studia/semestr_3_AM/Big_Data/Big_Data/projekt/sample_images/\"\n",
    "else:\n",
    "    results_path = \"/home/olek/studia/semestr_3_AM/Big_Data/Big_Data/projekt/results/\"\n",
    "    images_path = \"/home/olek/studia/semestr_3_AM/Big_Data/Big_Data/projekt/images/\"\n",
    "\n",
    "seed = 777\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(\"/home/olek/studia/semestr_3_AM/Big_Data/Big_Data/projekt/data/charts.csv\", nrows=10**6)\\\n",
    "#     .to_csv(\"/home/olek/studia/semestr_3_AM/Big_Data/Big_Data/projekt/data/sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/25 01:24:07 WARN Utils: Your hostname, olek resolves to a loopback address: 127.0.1.1; using 192.168.100.7 instead (on interface wlp6s0)\n",
      "24/06/25 01:24:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/25 01:24:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/home/olek/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Results\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "df = df.withColumn(\"rank\", f.col(\"rank\").cast(t.IntegerType())).withColumn(\"date\", f.col(\"date\").cast(t.DateType())).withColumn(\"streams\", f.col(\"streams\").cast(t.IntegerType()))\n",
    "df.registerTempTable(\"charts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- rank: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- chart: string (nullable = true)\n",
      " |-- trend: string (nullable = true)\n",
      " |-- streams: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show head data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = spark.sql(\"SELECT * FROM charts LIMIT 10\")\n",
    "with open(results_path + \"head.pkl\", \"wb\") as f:\n",
    "    pickle.dump(head.toPandas(), f)\n",
    "    \n",
    "df_styled = spark.sql(\"SELECT * from charts limit 10\").toPandas().style.background_gradient() \n",
    "dfi.export(df_styled, images_path + \"mytable.png\")\n",
    "df_styled = head.toPandas().style.background_gradient() \n",
    "dfi.export(df_styled, images_path + \"head.png\")\n",
    "head.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rank</th>\n",
       "      <th>date</th>\n",
       "      <th>artist</th>\n",
       "      <th>url</th>\n",
       "      <th>region</th>\n",
       "      <th>chart</th>\n",
       "      <th>trend</th>\n",
       "      <th>streams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chantaje (feat. Maluma)</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>https://open.spotify.com/track/6mICuAdrwEjh6Y6...</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>top200</td>\n",
       "      <td>SAME_POSITION</td>\n",
       "      <td>253019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vente Pa' Ca (feat. Maluma)</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Ricky Martin</td>\n",
       "      <td>https://open.spotify.com/track/7DM4BPaS7uofFul...</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>top200</td>\n",
       "      <td>MOVE_UP</td>\n",
       "      <td>223988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reggaet√≥n Lento (Bailemos)</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>CNCO</td>\n",
       "      <td>https://open.spotify.com/track/3AEZUABDXNtecAO...</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>top200</td>\n",
       "      <td>MOVE_DOWN</td>\n",
       "      <td>210943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Safari</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>J Balvin, Pharrell Williams, BIA, Sky</td>\n",
       "      <td>https://open.spotify.com/track/6rQSrBHf7HlZjtc...</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>top200</td>\n",
       "      <td>SAME_POSITION</td>\n",
       "      <td>173865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shaky Shaky</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Daddy Yankee</td>\n",
       "      <td>https://open.spotify.com/track/58IL315gMSTD37D...</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>top200</td>\n",
       "      <td>MOVE_UP</td>\n",
       "      <td>153956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Traicionera</td>\n",
       "      <td>6</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Sebastian Yatra</td>\n",
       "      <td>https://open.spotify.com/track/5J1c3M4EldCfNxX...</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>top200</td>\n",
       "      <td>MOVE_DOWN</td>\n",
       "      <td>151140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cuando Se Pone a Bailar</td>\n",
       "      <td>7</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Rombai</td>\n",
       "      <td>https://open.spotify.com/track/1MpKZi1zTXpERKw...</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>top200</td>\n",
       "      <td>MOVE_DOWN</td>\n",
       "      <td>148369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Otra vez (feat. J Balvin)</td>\n",
       "      <td>8</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Zion &amp; Lennox</td>\n",
       "      <td>https://open.spotify.com/track/3QwBODjSEzelZyV...</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>top200</td>\n",
       "      <td>MOVE_DOWN</td>\n",
       "      <td>143004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>La Bicicleta</td>\n",
       "      <td>9</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Carlos Vives, Shakira</td>\n",
       "      <td>https://open.spotify.com/track/0sXvAOmXgjR2QUq...</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>top200</td>\n",
       "      <td>MOVE_UP</td>\n",
       "      <td>126389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dile Que Tu Me Quieres</td>\n",
       "      <td>10</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Ozuna</td>\n",
       "      <td>https://open.spotify.com/track/20ZAJdsKB5IGbGj...</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>top200</td>\n",
       "      <td>MOVE_DOWN</td>\n",
       "      <td>112012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         title  rank        date  \\\n",
       "0      Chantaje (feat. Maluma)     1  2017-01-01   \n",
       "1  Vente Pa' Ca (feat. Maluma)     2  2017-01-01   \n",
       "2   Reggaet√≥n Lento (Bailemos)     3  2017-01-01   \n",
       "3                       Safari     4  2017-01-01   \n",
       "4                  Shaky Shaky     5  2017-01-01   \n",
       "5                  Traicionera     6  2017-01-01   \n",
       "6      Cuando Se Pone a Bailar     7  2017-01-01   \n",
       "7    Otra vez (feat. J Balvin)     8  2017-01-01   \n",
       "8                 La Bicicleta     9  2017-01-01   \n",
       "9       Dile Que Tu Me Quieres    10  2017-01-01   \n",
       "\n",
       "                                  artist  \\\n",
       "0                                Shakira   \n",
       "1                           Ricky Martin   \n",
       "2                                   CNCO   \n",
       "3  J Balvin, Pharrell Williams, BIA, Sky   \n",
       "4                           Daddy Yankee   \n",
       "5                        Sebastian Yatra   \n",
       "6                                 Rombai   \n",
       "7                          Zion & Lennox   \n",
       "8                  Carlos Vives, Shakira   \n",
       "9                                  Ozuna   \n",
       "\n",
       "                                                 url     region   chart  \\\n",
       "0  https://open.spotify.com/track/6mICuAdrwEjh6Y6...  Argentina  top200   \n",
       "1  https://open.spotify.com/track/7DM4BPaS7uofFul...  Argentina  top200   \n",
       "2  https://open.spotify.com/track/3AEZUABDXNtecAO...  Argentina  top200   \n",
       "3  https://open.spotify.com/track/6rQSrBHf7HlZjtc...  Argentina  top200   \n",
       "4  https://open.spotify.com/track/58IL315gMSTD37D...  Argentina  top200   \n",
       "5  https://open.spotify.com/track/5J1c3M4EldCfNxX...  Argentina  top200   \n",
       "6  https://open.spotify.com/track/1MpKZi1zTXpERKw...  Argentina  top200   \n",
       "7  https://open.spotify.com/track/3QwBODjSEzelZyV...  Argentina  top200   \n",
       "8  https://open.spotify.com/track/0sXvAOmXgjR2QUq...  Argentina  top200   \n",
       "9  https://open.spotify.com/track/20ZAJdsKB5IGbGj...  Argentina  top200   \n",
       "\n",
       "           trend  streams  \n",
       "0  SAME_POSITION   253019  \n",
       "1        MOVE_UP   223988  \n",
       "2      MOVE_DOWN   210943  \n",
       "3  SAME_POSITION   173865  \n",
       "4        MOVE_UP   153956  \n",
       "5      MOVE_DOWN   151140  \n",
       "6      MOVE_DOWN   148369  \n",
       "7      MOVE_DOWN   143004  \n",
       "8        MOVE_UP   126389  \n",
       "9      MOVE_DOWN   112012  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------+---+------+-----+-----+-------+\n",
      "|title|rank|date|artist|url|region|chart|trend|streams|\n",
      "+-----+----+----+------+---+------+-----+-----+-------+\n",
      "|    0|2142|2142|     0|  0|     0|    0|    0|5855331|\n",
      "+-----+----+----+------+---+------+-----+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 3min 32s\n",
    "columns = df.columns\n",
    "missing_data = df.select([(sum(col(c).isNull().cast(\"int\")).alias(c)) for c in columns])\n",
    "missing_data.show()\n",
    "\n",
    "with open(results_path + \"missing_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(missing_data.toPandas(), f)\n",
    "\n",
    "df_styled = missing_data.toPandas().style.background_gradient()\n",
    "dfi.export(df_styled, images_path + \"missing_data.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 ¬µs, sys: 2 ¬µs, total: 8 ¬µs\n",
      "Wall time: 12.6 ¬µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# time 8min 26s\n",
    "missing_streams_df = df.filter(col('streams').isNull())\n",
    "unique_values = {col_name: missing_streams_df.select(col_name).distinct().count() for col_name in missing_streams_df.columns}\n",
    "unique_values_df = pd.DataFrame(unique_values, index=[0])\n",
    "\n",
    "with open(results_path + \"unique_values_df.pkl\", \"wb\") as f:\n",
    "    pickle.dump(unique_values_df, f)\n",
    "\n",
    "df_styled = unique_values_df.style.background_gradient() \n",
    "dfi.export(df_styled, images_path + \"unique_values.png\")\n",
    "unique_values_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rank</th>\n",
       "      <th>date</th>\n",
       "      <th>artist</th>\n",
       "      <th>url</th>\n",
       "      <th>region</th>\n",
       "      <th>chart</th>\n",
       "      <th>trend</th>\n",
       "      <th>streams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119407</td>\n",
       "      <td>201</td>\n",
       "      <td>1827</td>\n",
       "      <td>81938</td>\n",
       "      <td>148226</td>\n",
       "      <td>208</td>\n",
       "      <td>102</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    title  rank  date  artist     url  region  chart  trend  streams\n",
       "0  119407   201  1827   81938  148226     208    102     68        1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count np of missing striming value per chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 ¬µs, sys: 4 ¬µs, total: 18 ¬µs\n",
      "Wall time: 498 ¬µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Your Styled DataFrame has more than 100 rows and will produce a huge image file, possibly causing your computer to crash. Override this error by explicitly setting `max_rows` to -1 for all columns. Styled DataFrames are unable to select a subset of rows or columns and therefore do not work with the `max_rows` and `max_cols` parameters",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(null_cart_df, f)\n\u001b[1;32m     12\u001b[0m df_styled \u001b[38;5;241m=\u001b[39m null_cart_df\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39mbackground_gradient() \n\u001b[0;32m---> 13\u001b[0m \u001b[43mdfi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_styled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munique_values.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dataframe_image/_pandas_accessor.py:115\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(obj, filename, fontsize, max_rows, max_cols, table_conversion, chrome_path, dpi, use_mathjax)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_styler:\n\u001b[1;32m    108\u001b[0m         error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    109\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour Styled DataFrame has more than \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_ROWS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows and will produce \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma huge image file, possibly causing your computer to crash. Override \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand therefore do not work with the `max_rows` and `max_cols` parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         )\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m MAX_COLS \u001b[38;5;129;01mand\u001b[39;00m max_cols \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour DataFrame has more than \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_COLS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns and will produce a huge \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file, possibly causing your computer to crash. Override this error \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby explicitly setting `max_cols`. Use -1 for all columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Your Styled DataFrame has more than 100 rows and will produce a huge image file, possibly causing your computer to crash. Override this error by explicitly setting `max_rows` to -1 for all columns. Styled DataFrames are unable to select a subset of rows or columns and therefore do not work with the `max_rows` and `max_cols` parameters"
     ]
    }
   ],
   "source": [
    "%time\n",
    "null_cart = spark.sql(\"\"\"\n",
    "    SELECT chart, count(chart) as count\n",
    "    FROM charts\n",
    "    WHERE streams IS NULL\n",
    "    GROUP BY chart\n",
    "    ORDER BY count DESC;\"\"\")\n",
    "null_cart_df = null_cart.toPandas()\n",
    "with open(results_path + \"null_cart_df.pkl\", \"wb\") as f:\n",
    "    pickle.dump(null_cart_df, f)\n",
    "\n",
    "# df_styled = null_cart_df.style.background_gradient() \n",
    "# dfi.export(df_styled, images_path + \"unique_values.png\")\n",
    "null_cart_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chart</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>viral50</td>\n",
       "      <td>5848001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paraguay</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chile</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://open.spotify.com/track/0xsAtU9TemtJgyV...</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Cosculluela\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Russia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>https://open.spotify.com/track/70wb5cVwwdHzvpt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>https://open.spotify.com/track/4p80VsLJul7HDkx...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Latvia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 chart    count\n",
       "0                                              viral50  5848001\n",
       "1                                             Paraguay      704\n",
       "2                                              Germany      555\n",
       "3                                                Chile      516\n",
       "4    https://open.spotify.com/track/0xsAtU9TemtJgyV...      438\n",
       "..                                                 ...      ...\n",
       "97                                        Cosculluela\"        1\n",
       "98                                              Russia        1\n",
       "99   https://open.spotify.com/track/70wb5cVwwdHzvpt...        1\n",
       "100  https://open.spotify.com/track/4p80VsLJul7HDkx...        1\n",
       "101                                             Latvia        1\n",
       "\n",
       "[102 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SElect TOP200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 ¬µs, sys: 1 ¬µs, total: 7 ¬µs\n",
      "Wall time: 14.3 ¬µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----------+--------------------+--------------------+---------+------+-------------+-------+\n",
      "|               title|rank|      date|              artist|                 url|   region| chart|        trend|streams|\n",
      "+--------------------+----+----------+--------------------+--------------------+---------+------+-------------+-------+\n",
      "|Chantaje (feat. M...|   1|2017-01-01|             Shakira|https://open.spot...|Argentina|top200|SAME_POSITION| 253019|\n",
      "|Vente Pa' Ca (fea...|   2|2017-01-01|        Ricky Martin|https://open.spot...|Argentina|top200|      MOVE_UP| 223988|\n",
      "|Reggaet√≥n Lento (...|   3|2017-01-01|                CNCO|https://open.spot...|Argentina|top200|    MOVE_DOWN| 210943|\n",
      "|              Safari|   4|2017-01-01|J Balvin, Pharrel...|https://open.spot...|Argentina|top200|SAME_POSITION| 173865|\n",
      "|         Shaky Shaky|   5|2017-01-01|        Daddy Yankee|https://open.spot...|Argentina|top200|      MOVE_UP| 153956|\n",
      "|         Traicionera|   6|2017-01-01|     Sebastian Yatra|https://open.spot...|Argentina|top200|    MOVE_DOWN| 151140|\n",
      "|Cuando Se Pone a ...|   7|2017-01-01|              Rombai|https://open.spot...|Argentina|top200|    MOVE_DOWN| 148369|\n",
      "|Otra vez (feat. J...|   8|2017-01-01|       Zion & Lennox|https://open.spot...|Argentina|top200|    MOVE_DOWN| 143004|\n",
      "|        La Bicicleta|   9|2017-01-01|Carlos Vives, Sha...|https://open.spot...|Argentina|top200|      MOVE_UP| 126389|\n",
      "|Dile Que Tu Me Qu...|  10|2017-01-01|               Ozuna|https://open.spot...|Argentina|top200|    MOVE_DOWN| 112012|\n",
      "|  Andas En Mi Cabeza|  11|2017-01-01|Chino & Nacho, Da...|https://open.spot...|Argentina|top200|SAME_POSITION| 110395|\n",
      "|Desde Esa Noche (...|  12|2017-01-01|              Thalia|https://open.spot...|Argentina|top200|      MOVE_UP| 104592|\n",
      "|      Borro Cassette|  13|2017-01-01|              Maluma|https://open.spot...|Argentina|top200|      MOVE_UP| 101535|\n",
      "|Gyal You A Party ...|  14|2017-01-01|Charly Black, Dad...|https://open.spot...|Argentina|top200|    MOVE_DOWN|  99722|\n",
      "|Me llamas (feat. ...|  15|2017-01-01|             Piso 21|https://open.spot...|Argentina|top200|    MOVE_DOWN|  95010|\n",
      "|La Bicicleta (fea...|  16|2017-01-01|Carlos Vives, Sha...|https://open.spot...|Argentina|top200|      MOVE_UP|  92723|\n",
      "|DUELE EL CORAZON ...|  17|2017-01-01|    Enrique Iglesias|https://open.spot...|Argentina|top200|      MOVE_UP|  91325|\n",
      "|     Let Me Love You|  18|2017-01-01|DJ Snake, Justin ...|https://open.spot...|Argentina|top200|    MOVE_DOWN|  87926|\n",
      "|La Noche No Es pa...|  19|2017-01-01|         Mano Arriba|https://open.spot...|Argentina|top200|SAME_POSITION|  87033|\n",
      "|          Vacaciones|  20|2017-01-01|               Wisin|https://open.spot...|Argentina|top200|    MOVE_DOWN|  86103|\n",
      "+--------------------+----+----------+--------------------+--------------------+---------+------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "filtered_df = spark.sql(\"SELECT * FROM charts WHERE chart = 'top200'\")\n",
    "filtered_df.show()\n",
    "df = filtered_df\n",
    "df.createOrReplaceTempView(\"charts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 ¬µs, sys: 1 ¬µs, total: 8 ¬µs\n",
      "Wall time: 13.8 ¬µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NoOfObservationsTop200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20318183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NoOfObservationsTop200\n",
       "0                20318183"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "spark.sql('''\n",
    "SELECT COUNT(*) NoOfObservationsTop200\n",
    "FROM charts;\n",
    "''').toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ¬µs, sys: 1 ¬µs, total: 5 ¬µs\n",
      "Wall time: 8.82 ¬µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------+---+------+-----+-----+-------+\n",
      "|title|rank|date|artist|url|region|chart|trend|streams|\n",
      "+-----+----+----+------+---+------+-----+-----+-------+\n",
      "|    0|   0|   0|     0|  0|     0|    0|    0|      0|\n",
      "+-----+----+----+------+---+------+-----+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%time\n",
    "columns = df.columns\n",
    "missing_data = df.select([(sum(col(c).isNull().cast(\"int\")).alias(c)) for c in columns])\n",
    "missing_data.show()\n",
    "\n",
    "with open(results_path + \"missing_data_clearing.pkl\", \"wb\") as f:\n",
    "    pickle.dump(missing_data.toPandas(), f)\n",
    "\n",
    "df_styled = missing_data.toPandas().style.background_gradient()\n",
    "dfi.export(df_styled, images_path + \"missing_data.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 ¬µs, sys: 4 ¬µs, total: 15 ¬µs\n",
      "Wall time: 25.7 ¬µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+\n",
      "| max(date)| min(date)|          intervale|\n",
      "+----------+----------+-------------------+\n",
      "|2021-12-31|2017-01-01|INTERVAL '1825' DAY|\n",
      "+----------+----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max(date)</th>\n",
       "      <th>min(date)</th>\n",
       "      <th>intervale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>1825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max(date)   min(date)  intervale\n",
       "0  2021-12-31  2017-01-01       1825"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "data_intervale = spark.sql(\"SELECT  MAX(date), MIN(date), MAX(date) - MIN(date) as intervale FROM charts\")\n",
    "data_intervale.show()\n",
    "data_intervale_df = data_intervale.toPandas() \n",
    "full_data = data_intervale_df[\"intervale\"][0].days\n",
    "\n",
    "with open(results_path + \"data_intervale.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_intervale_df, f)\n",
    "\n",
    "df_styled = data_intervale.toPandas()\n",
    "df_styled[\"intervale\"] = full_data\n",
    "df_styled.style.background_gradient()\n",
    "dfi.export(df_styled, images_path + \"data_intervale.png\")\n",
    "df_styled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afrer clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## integrity ratio by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 ¬µs, sys: 0 ns, total: 6 ¬µs\n",
      "Wall time: 11 ¬µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fe6a7_row0_col1, #T_fe6a7_row0_col2, #T_fe6a7_row1_col1, #T_fe6a7_row1_col2, #T_fe6a7_row2_col1, #T_fe6a7_row2_col2, #T_fe6a7_row3_col1, #T_fe6a7_row3_col2, #T_fe6a7_row3_col3 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_fe6a7_row0_col3 {\n",
       "  background-color: #76aad0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_fe6a7_row1_col3 {\n",
       "  background-color: #78abd0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_fe6a7_row2_col3 {\n",
       "  background-color: #3b92c1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_fe6a7_row4_col1, #T_fe6a7_row4_col2 {\n",
       "  background-color: #02395a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_fe6a7_row4_col3 {\n",
       "  background-color: #99b8d8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_fe6a7_row5_col1, #T_fe6a7_row5_col2 {\n",
       "  background-color: #023c5f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_fe6a7_row5_col3 {\n",
       "  background-color: #0567a2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_fe6a7_row6_col1, #T_fe6a7_row6_col2, #T_fe6a7_row7_col1, #T_fe6a7_row7_col2 {\n",
       "  background-color: #cdd0e5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_fe6a7_row6_col3 {\n",
       "  background-color: #dad9ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_fe6a7_row7_col3 {\n",
       "  background-color: #dedcec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_fe6a7_row8_col1, #T_fe6a7_row8_col2 {\n",
       "  background-color: #d2d3e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_fe6a7_row8_col3 {\n",
       "  background-color: #dbdaeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_fe6a7_row9_col1, #T_fe6a7_row9_col2 {\n",
       "  background-color: #e8e4f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_fe6a7_row9_col3, #T_fe6a7_row10_col1, #T_fe6a7_row10_col2 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_fe6a7_row10_col3 {\n",
       "  background-color: #f6eff7;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fe6a7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fe6a7_level0_col0\" class=\"col_heading level0 col0\" >region</th>\n",
       "      <th id=\"T_fe6a7_level0_col1\" class=\"col_heading level0 col1\" >count</th>\n",
       "      <th id=\"T_fe6a7_level0_col2\" class=\"col_heading level0 col2\" >procent</th>\n",
       "      <th id=\"T_fe6a7_level0_col3\" class=\"col_heading level0 col3\" >artist_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fe6a7_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_fe6a7_row0_col0\" class=\"data row0 col0\" >Brazil</td>\n",
       "      <td id=\"T_fe6a7_row0_col1\" class=\"data row0 col1\" >364516</td>\n",
       "      <td id=\"T_fe6a7_row0_col2\" class=\"data row0 col2\" >0.998674</td>\n",
       "      <td id=\"T_fe6a7_row0_col3\" class=\"data row0 col3\" >1764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe6a7_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_fe6a7_row1_col0\" class=\"data row1 col0\" >Argentina</td>\n",
       "      <td id=\"T_fe6a7_row1_col1\" class=\"data row1 col1\" >364326</td>\n",
       "      <td id=\"T_fe6a7_row1_col2\" class=\"data row1 col2\" >0.998153</td>\n",
       "      <td id=\"T_fe6a7_row1_col3\" class=\"data row1 col3\" >1754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe6a7_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_fe6a7_row2_col0\" class=\"data row2 col0\" >United States</td>\n",
       "      <td id=\"T_fe6a7_row2_col1\" class=\"data row2 col1\" >364157</td>\n",
       "      <td id=\"T_fe6a7_row2_col2\" class=\"data row2 col2\" >0.997690</td>\n",
       "      <td id=\"T_fe6a7_row2_col3\" class=\"data row2 col3\" >2068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe6a7_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_fe6a7_row3_col0\" class=\"data row3 col0\" >Austria</td>\n",
       "      <td id=\"T_fe6a7_row3_col1\" class=\"data row3 col1\" >363810</td>\n",
       "      <td id=\"T_fe6a7_row3_col2\" class=\"data row3 col2\" >0.996740</td>\n",
       "      <td id=\"T_fe6a7_row3_col3\" class=\"data row3 col3\" >3035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe6a7_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_fe6a7_row4_col0\" class=\"data row4 col0\" >Australia</td>\n",
       "      <td id=\"T_fe6a7_row4_col1\" class=\"data row4 col1\" >362181</td>\n",
       "      <td id=\"T_fe6a7_row4_col2\" class=\"data row4 col2\" >0.992277</td>\n",
       "      <td id=\"T_fe6a7_row4_col3\" class=\"data row4 col3\" >1558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe6a7_level0_row5\" class=\"row_heading level0 row5\" >26</th>\n",
       "      <td id=\"T_fe6a7_row5_col0\" class=\"data row5 col0\" >Poland</td>\n",
       "      <td id=\"T_fe6a7_row5_col1\" class=\"data row5 col1\" >358189</td>\n",
       "      <td id=\"T_fe6a7_row5_col2\" class=\"data row5 col2\" >0.981340</td>\n",
       "      <td id=\"T_fe6a7_row5_col3\" class=\"data row5 col3\" >2528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe6a7_level0_row6\" class=\"row_heading level0 row6\" >64</th>\n",
       "      <td id=\"T_fe6a7_row6_col0\" class=\"data row6 col0\" >Russia</td>\n",
       "      <td id=\"T_fe6a7_row6_col1\" class=\"data row6 col1\" >100767</td>\n",
       "      <td id=\"T_fe6a7_row6_col2\" class=\"data row6 col2\" >0.276074</td>\n",
       "      <td id=\"T_fe6a7_row6_col3\" class=\"data row6 col3\" >1041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe6a7_level0_row7\" class=\"row_heading level0 row7\" >65</th>\n",
       "      <td id=\"T_fe6a7_row7_col0\" class=\"data row7 col0\" >Ukraine</td>\n",
       "      <td id=\"T_fe6a7_row7_col1\" class=\"data row7 col1\" >100766</td>\n",
       "      <td id=\"T_fe6a7_row7_col2\" class=\"data row7 col2\" >0.276071</td>\n",
       "      <td id=\"T_fe6a7_row7_col3\" class=\"data row7 col3\" >1010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe6a7_level0_row8\" class=\"row_heading level0 row8\" >66</th>\n",
       "      <td id=\"T_fe6a7_row8_col0\" class=\"data row8 col0\" >Bulgaria</td>\n",
       "      <td id=\"T_fe6a7_row8_col1\" class=\"data row8 col1\" >94834</td>\n",
       "      <td id=\"T_fe6a7_row8_col2\" class=\"data row8 col2\" >0.259819</td>\n",
       "      <td id=\"T_fe6a7_row8_col3\" class=\"data row8 col3\" >1033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe6a7_level0_row9\" class=\"row_heading level0 row9\" >67</th>\n",
       "      <td id=\"T_fe6a7_row9_col0\" class=\"data row9 col0\" >South Korea</td>\n",
       "      <td id=\"T_fe6a7_row9_col1\" class=\"data row9 col1\" >59576</td>\n",
       "      <td id=\"T_fe6a7_row9_col2\" class=\"data row9 col2\" >0.163222</td>\n",
       "      <td id=\"T_fe6a7_row9_col3\" class=\"data row9 col3\" >531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe6a7_level0_row10\" class=\"row_heading level0 row10\" >68</th>\n",
       "      <td id=\"T_fe6a7_row10_col0\" class=\"data row10 col0\" >Luxembourg</td>\n",
       "      <td id=\"T_fe6a7_row10_col1\" class=\"data row10 col1\" >8962</td>\n",
       "      <td id=\"T_fe6a7_row10_col2\" class=\"data row10 col2\" >0.024553</td>\n",
       "      <td id=\"T_fe6a7_row10_col3\" class=\"data row10 col3\" >684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x79c37db40520>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "procentag_region = spark.sql(\"\"\"\n",
    "    SELECT region, COUNT(region) as count, COUNT(region) / {} as procent, COUNT( DISTINCT artist) as artist_count\n",
    "    FROM charts\n",
    "    GROUP BY region\n",
    "    ORDER BY count DESC;\"\"\".format(full_data*200))\n",
    "procentag_region = procentag_region.toPandas()\n",
    "\n",
    "with open(results_path + \"procentag_region.pkl\", \"wb\") as f:\n",
    "    pickle.dump(procentag_region, f)\n",
    "\n",
    "procentag_region_filtr = pd.concat([procentag_region.head(5), procentag_region[procentag_region[\"region\"] == \"Poland\"] , procentag_region.tail(5)])\n",
    "df_styled = procentag_region_filtr.style.background_gradient()\n",
    "dfi.export(df_styled, images_path + \"procentag_region.png\")\n",
    "df_styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(False, 0.005).toPandas().to_csv(results_path + \"sample_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## artist_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "artist_counts = df.groupBy('artist').agg(count('title').alias('song_count')).orderBy(col('song_count').desc())\n",
    "artist_counts_pd = artist_counts.toPandas()\n",
    "with open(results_path + \"artist_counts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(artist_counts_pd, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>region</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mar√≠lia Mendon√ßa</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>11059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Juice WRLD</td>\n",
       "      <td>United States</td>\n",
       "      <td>10524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Post Malone</td>\n",
       "      <td>United States</td>\n",
       "      <td>10436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Z√© Neto &amp; Cristiano</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>9385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Henrique &amp; Juliano</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>8914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8109</th>\n",
       "      <td>La Roux</td>\n",
       "      <td>Poland</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8110</th>\n",
       "      <td>Soft Cell</td>\n",
       "      <td>United States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8111</th>\n",
       "      <td>Manu Gavassi, Voyou</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8112</th>\n",
       "      <td>Ellie Nelson</td>\n",
       "      <td>United States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8113</th>\n",
       "      <td>Carlie Hanson</td>\n",
       "      <td>United States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8114 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   artist         region  count\n",
       "0        Mar√≠lia Mendon√ßa         Brazil  11059\n",
       "1              Juice WRLD  United States  10524\n",
       "2             Post Malone  United States  10436\n",
       "3     Z√© Neto & Cristiano         Brazil   9385\n",
       "4      Henrique & Juliano         Brazil   8914\n",
       "...                   ...            ...    ...\n",
       "8109              La Roux         Poland      1\n",
       "8110            Soft Cell  United States      1\n",
       "8111  Manu Gavassi, Voyou         Brazil      1\n",
       "8112         Ellie Nelson  United States      1\n",
       "8113        Carlie Hanson  United States      1\n",
       "\n",
       "[8114 rows x 3 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions = procentag_region_filtr[:3][\"region\"].to_list()+[\"Poland\"]\n",
    "region_top_artist = spark.sql(\"\"\"\n",
    "    SELECT artist, region, COUNT(artist) as count\n",
    "    FROM charts\n",
    "    WHERE region IN {}\n",
    "    GROUP BY region, artist\n",
    "    ORDER BY count DESC;\n",
    "\"\"\".format(tuple(regions)))\n",
    "region_top_artist_df = region_top_artist.toPandas()\n",
    "with open(results_path + \"region_top_artist.pkl\", \"wb\") as f:\n",
    "    pickle.dump(region_top_artist_df, f)\n",
    "region_top_artist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add year, mont, top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----------+--------------------+--------------------+---------+------+-------------+-------+----+-----+-----+\n",
      "|               title|rank|      date|              artist|                 url|   region| chart|        trend|streams|year|month|top10|\n",
      "+--------------------+----+----------+--------------------+--------------------+---------+------+-------------+-------+----+-----+-----+\n",
      "|Chantaje (feat. M...|   1|2017-01-01|             Shakira|https://open.spot...|Argentina|top200|SAME_POSITION| 253019|2017|    1| true|\n",
      "|Vente Pa' Ca (fea...|   2|2017-01-01|        Ricky Martin|https://open.spot...|Argentina|top200|      MOVE_UP| 223988|2017|    1| true|\n",
      "|Reggaet√≥n Lento (...|   3|2017-01-01|                CNCO|https://open.spot...|Argentina|top200|    MOVE_DOWN| 210943|2017|    1| true|\n",
      "|              Safari|   4|2017-01-01|J Balvin, Pharrel...|https://open.spot...|Argentina|top200|SAME_POSITION| 173865|2017|    1| true|\n",
      "|         Shaky Shaky|   5|2017-01-01|        Daddy Yankee|https://open.spot...|Argentina|top200|      MOVE_UP| 153956|2017|    1| true|\n",
      "|         Traicionera|   6|2017-01-01|     Sebastian Yatra|https://open.spot...|Argentina|top200|    MOVE_DOWN| 151140|2017|    1| true|\n",
      "|Cuando Se Pone a ...|   7|2017-01-01|              Rombai|https://open.spot...|Argentina|top200|    MOVE_DOWN| 148369|2017|    1| true|\n",
      "|Otra vez (feat. J...|   8|2017-01-01|       Zion & Lennox|https://open.spot...|Argentina|top200|    MOVE_DOWN| 143004|2017|    1| true|\n",
      "|        La Bicicleta|   9|2017-01-01|Carlos Vives, Sha...|https://open.spot...|Argentina|top200|      MOVE_UP| 126389|2017|    1| true|\n",
      "|Dile Que Tu Me Qu...|  10|2017-01-01|               Ozuna|https://open.spot...|Argentina|top200|    MOVE_DOWN| 112012|2017|    1|false|\n",
      "|  Andas En Mi Cabeza|  11|2017-01-01|Chino & Nacho, Da...|https://open.spot...|Argentina|top200|SAME_POSITION| 110395|2017|    1|false|\n",
      "|Desde Esa Noche (...|  12|2017-01-01|              Thalia|https://open.spot...|Argentina|top200|      MOVE_UP| 104592|2017|    1|false|\n",
      "|      Borro Cassette|  13|2017-01-01|              Maluma|https://open.spot...|Argentina|top200|      MOVE_UP| 101535|2017|    1|false|\n",
      "|Gyal You A Party ...|  14|2017-01-01|Charly Black, Dad...|https://open.spot...|Argentina|top200|    MOVE_DOWN|  99722|2017|    1|false|\n",
      "|Me llamas (feat. ...|  15|2017-01-01|             Piso 21|https://open.spot...|Argentina|top200|    MOVE_DOWN|  95010|2017|    1|false|\n",
      "|La Bicicleta (fea...|  16|2017-01-01|Carlos Vives, Sha...|https://open.spot...|Argentina|top200|      MOVE_UP|  92723|2017|    1|false|\n",
      "|DUELE EL CORAZON ...|  17|2017-01-01|    Enrique Iglesias|https://open.spot...|Argentina|top200|      MOVE_UP|  91325|2017|    1|false|\n",
      "|     Let Me Love You|  18|2017-01-01|DJ Snake, Justin ...|https://open.spot...|Argentina|top200|    MOVE_DOWN|  87926|2017|    1|false|\n",
      "|La Noche No Es pa...|  19|2017-01-01|         Mano Arriba|https://open.spot...|Argentina|top200|SAME_POSITION|  87033|2017|    1|false|\n",
      "|          Vacaciones|  20|2017-01-01|               Wisin|https://open.spot...|Argentina|top200|    MOVE_DOWN|  86103|2017|    1|false|\n",
      "+--------------------+----+----------+--------------------+--------------------+---------+------+-------------+-------+----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_songs = spark.sql(\"\"\"\n",
    "    SELECT title, rank, date, artist, url, region, chart, trend, streams, year(date) as year, month(date) as month, rank<10 as top10\n",
    "    FROM charts;\"\"\")\n",
    "df = df_songs\n",
    "df.show()\n",
    "df.createOrReplaceTempView(\"charts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## total streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "strem_in_years = spark.sql('''\n",
    "SELECT year, month, sum(streams) as streams\n",
    "FROM charts\n",
    "GROUP BY year, month\n",
    "ORDER BY year, month;\n",
    "''')\n",
    "strem_per_yers_pd = strem_in_years.toPandas()\n",
    "with open(results_path + \"strem_per_year.pkl\", \"wb\") as f:\n",
    "    pickle.dump(strem_per_yers_pd, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## most_popoular_aritst_in_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "most_popoular_aritst_in_year = spark.sql(\"\"\"\n",
    "SELECT year, artist, sum(streams) as streams\n",
    "FROM charts\n",
    "GROUP BY year, artist\n",
    "ORDER BY year, streams DESC;\n",
    "\"\"\")\n",
    "with open(results_path + \"most_popoular_aritst_in_year.pkl\", \"wb\") as f:\n",
    "    pickle.dump(most_popoular_aritst_in_year.toPandas(), f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## market_share_development "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "market_share_development = spark.sql(\"\"\"\n",
    "SELECT year, region, sum(streams) as streams\n",
    "FROM charts\n",
    "WHERE region != 'Global'\n",
    "GROUP BY year, region\n",
    "ORDER BY year, region;\n",
    "\"\"\")\n",
    "market_share_development_pd = market_share_development.toPandas()\n",
    "with open(results_path + \"market_share_development.pkl\", \"wb\") as f:\n",
    "    pickle.dump(market_share_development_pd, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## avg rank by regon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "avg_rank_by_region_pd = spark.sql(\"\"\"\n",
    "SELECT region, avg(rank) as avg_rank\n",
    "FROM charts\n",
    "GROUP BY region\n",
    "ORDER BY avg_rank;\n",
    "\"\"\")\n",
    "avg_rank_by_region_pd = avg_rank_by_region_pd.toPandas()\n",
    "with open(results_path + \"avg_rank_by_region_of_best_song.pkl\", \"wb\") as f:\n",
    "    pickle.dump(avg_rank_by_region_pd, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## best songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.][Stage 8:>   (0 + 0) / 26]6]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/olek/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/olek/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m best_song \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mSELECT title, sum(streams) as streams\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mFROM charts\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mGROUP BY title\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mORDER BY streams DESC;\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m best_song_pd \u001b[38;5;241m=\u001b[39m \u001b[43mbest_song\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(results_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_song.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(best_song_pd, f)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[1;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1261\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m \n\u001b[1;32m   1243\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1261\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_song = spark.sql(\"\"\"\n",
    "SELECT title, sum(streams) as streams\n",
    "FROM charts\n",
    "GROUP BY title\n",
    "ORDER BY streams DESC;\n",
    "\"\"\")\n",
    "best_song_pd = best_song.toPandas()\n",
    "with open(results_path + \"best_song.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_song_pd, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "best_song = spark.sql(\"\"\"\n",
    "SELECT title, count(title) as count, sum(streams) as streams, avg(rank) as avg_rank\n",
    "FROM charts\n",
    "GROUP BY title\n",
    "ORDER BY streams DESC;\n",
    "\"\"\")\n",
    "best_song_pd = best_song.toPandas()\n",
    "with open(results_path + \"best_song.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_song_pd, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "best_songs_hisotry = spark.sql(\"\"\"\n",
    "SELECT title, date, rank, streams\n",
    "FROM charts\n",
    "WHERE title IN {};\n",
    "\"\"\".format(tuple(best_song_pd[\"title\"].tolist()[:5])))\n",
    "best_songs_hisotry_pd = best_songs_hisotry.toPandas()\n",
    "with open(results_path + \"best_songs_hisotry.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_songs_hisotry_pd, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# avg rank by region of best song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_song_titles = best_song_pd[\"title\"].to_list()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "avg_rank_by_region_pd = spark.sql(\"\"\"\n",
    "SELECT region, avg(rank) as avg_rank\n",
    "FROM charts\n",
    "WHERE title IN {}\n",
    "GROUP BY region\n",
    "ORDER BY avg_rank;\n",
    "\"\"\".format(tuple(best_song_titles)))\n",
    "avg_rank_by_region_pd = avg_rank_by_region_pd.toPandas()\n",
    "with open(results_path + \"avg_rank_by_region_of_best_song.pkl\", \"wb\") as f:\n",
    "    pickle.dump(avg_rank_by_region_pd, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artist count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# artist_count = spark.sql(\"\"\"\n",
    "# # SELECT artist,\n",
    "# #     count(artist) as count,\n",
    "# #     sum(streams) as streams,\n",
    "# #     avg(rank) as avg_rank,\n",
    "# #     avg(rank) as avg_rank,\n",
    "# #     max(rank) as max_rank,\n",
    "# #     min(rank) as min_rank,\n",
    "# #     sum(rank)/count(artist) as avg_rank_per_song,\n",
    "# #     sum(streams)/count(artist) as avg_streams_per_song, \n",
    "# # FROM charts\n",
    "# # GROUP BY artist\n",
    "# # ORDER BY count DESC;\n",
    "# # \"\"\")\n",
    "artist_count = spark.sql(\"\"\"\n",
    "SELECT artist,\n",
    "    count(artist) as count\n",
    "FROM charts\n",
    "GROUP BY artist\n",
    "ORDER BY count DESC;\n",
    "\"\"\")\n",
    "\n",
    "artist_count_pd = artist_count.toPandas()\n",
    "with open(results_path + \"artist_count.pkl\", \"wb\") as f:\n",
    "    pickle.dump(artist_count_pd, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----------+--------------------+--------------------+---------+------+-------------+-------+\n",
      "|               title|rank|      date|              artist|                 url|   region| chart|        trend|streams|\n",
      "+--------------------+----+----------+--------------------+--------------------+---------+------+-------------+-------+\n",
      "|Chantaje (feat. M...|   1|2017-01-01|             Shakira|https://open.spot...|Argentina|top200|SAME_POSITION| 253019|\n",
      "|Vente Pa' Ca (fea...|   2|2017-01-01|        Ricky Martin|https://open.spot...|Argentina|top200|      MOVE_UP| 223988|\n",
      "|Reggaet√≥n Lento (...|   3|2017-01-01|                CNCO|https://open.spot...|Argentina|top200|    MOVE_DOWN| 210943|\n",
      "|              Safari|   4|2017-01-01|J Balvin, Pharrel...|https://open.spot...|Argentina|top200|SAME_POSITION| 173865|\n",
      "|         Shaky Shaky|   5|2017-01-01|        Daddy Yankee|https://open.spot...|Argentina|top200|      MOVE_UP| 153956|\n",
      "|         Traicionera|   6|2017-01-01|     Sebastian Yatra|https://open.spot...|Argentina|top200|    MOVE_DOWN| 151140|\n",
      "|Cuando Se Pone a ...|   7|2017-01-01|              Rombai|https://open.spot...|Argentina|top200|    MOVE_DOWN| 148369|\n",
      "|Otra vez (feat. J...|   8|2017-01-01|       Zion & Lennox|https://open.spot...|Argentina|top200|    MOVE_DOWN| 143004|\n",
      "|        La Bicicleta|   9|2017-01-01|Carlos Vives, Sha...|https://open.spot...|Argentina|top200|      MOVE_UP| 126389|\n",
      "|Dile Que Tu Me Qu...|  10|2017-01-01|               Ozuna|https://open.spot...|Argentina|top200|    MOVE_DOWN| 112012|\n",
      "|  Andas En Mi Cabeza|  11|2017-01-01|Chino & Nacho, Da...|https://open.spot...|Argentina|top200|SAME_POSITION| 110395|\n",
      "|Desde Esa Noche (...|  12|2017-01-01|              Thalia|https://open.spot...|Argentina|top200|      MOVE_UP| 104592|\n",
      "|      Borro Cassette|  13|2017-01-01|              Maluma|https://open.spot...|Argentina|top200|      MOVE_UP| 101535|\n",
      "|Gyal You A Party ...|  14|2017-01-01|Charly Black, Dad...|https://open.spot...|Argentina|top200|    MOVE_DOWN|  99722|\n",
      "|Me llamas (feat. ...|  15|2017-01-01|             Piso 21|https://open.spot...|Argentina|top200|    MOVE_DOWN|  95010|\n",
      "|La Bicicleta (fea...|  16|2017-01-01|Carlos Vives, Sha...|https://open.spot...|Argentina|top200|      MOVE_UP|  92723|\n",
      "|DUELE EL CORAZON ...|  17|2017-01-01|    Enrique Iglesias|https://open.spot...|Argentina|top200|      MOVE_UP|  91325|\n",
      "|     Let Me Love You|  18|2017-01-01|DJ Snake, Justin ...|https://open.spot...|Argentina|top200|    MOVE_DOWN|  87926|\n",
      "|La Noche No Es pa...|  19|2017-01-01|         Mano Arriba|https://open.spot...|Argentina|top200|SAME_POSITION|  87033|\n",
      "|          Vacaciones|  20|2017-01-01|               Wisin|https://open.spot...|Argentina|top200|    MOVE_DOWN|  86103|\n",
      "+--------------------+----+----------+--------------------+--------------------+---------+------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hukutaan</td>\n",
       "      <td>Arttu Lindeman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THE SCOTTS</td>\n",
       "      <td>THE SCOTTS, Travis Scott, Kid Cudi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Colocao</td>\n",
       "      <td>Nicki Nicole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Energie</td>\n",
       "      <td>Ronnie Flex, Frenna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tamagotchi</td>\n",
       "      <td>TACONAFIDE, Quebonafide, Taco Hemingway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2501</th>\n",
       "      <td>Off The Grid</td>\n",
       "      <td>Kanye West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>Longue vie</td>\n",
       "      <td>Sofiane, Ninho, Hornet La Frappe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>Geekd</td>\n",
       "      <td>Daniil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>N√∂ss√∂</td>\n",
       "      <td>Petri Nyg√•rd, Vesku Jokinen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>DNA</td>\n",
       "      <td>BTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2506 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             title                                   artist\n",
       "0         Hukutaan                           Arttu Lindeman\n",
       "1       THE SCOTTS       THE SCOTTS, Travis Scott, Kid Cudi\n",
       "2          Colocao                             Nicki Nicole\n",
       "3          Energie                      Ronnie Flex, Frenna\n",
       "4       Tamagotchi  TACONAFIDE, Quebonafide, Taco Hemingway\n",
       "...            ...                                      ...\n",
       "2501  Off The Grid                               Kanye West\n",
       "2502    Longue vie         Sofiane, Ninho, Hornet La Frappe\n",
       "2503         Geekd                                   Daniil\n",
       "2504         N√∂ss√∂              Petri Nyg√•rd, Vesku Jokinen\n",
       "2505           DNA                                      BTS\n",
       "\n",
       "[2506 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_one_songs = spark.sql(\"\"\"\n",
    "SELECT DISTINCT title, artist\n",
    "FROM charts\n",
    "WHERE rank = 1;\n",
    "\"\"\")         \n",
    "top_one_songs_pd = top_one_songs.toPandas()\n",
    "top_one_songs_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                (0 + 0) / 26]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/olek/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/olek/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m best_of_top_one_songs \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mSELECT title,\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    count(title) as count,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mORDER BY streams DESC;\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtuple\u001b[39m(top_one_songs_pd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list())))\n\u001b[0;32m---> 13\u001b[0m top_rank_one_songs_pd \u001b[38;5;241m=\u001b[39m \u001b[43mbest_of_top_one_songs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(results_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_rank_one_songs.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     15\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(top_rank_one_songs_pd, f)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[1;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1261\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m \n\u001b[1;32m   1243\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1261\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:====>             (6 + 4) / 26][Stage 7:>                 (0 + 0) / 26]\r"
     ]
    }
   ],
   "source": [
    "best_of_top_one_songs = spark.sql(\"\"\"\n",
    "SELECT title,\n",
    "    count(title) as count,\n",
    "    sum(streams) as streams,\n",
    "    avg(streams) as avg_streams,  \n",
    "    avg(rank) as avg_rank\n",
    "FROM charts\n",
    "WHERE title in {}\n",
    "GROUP BY title\n",
    "ORDER BY streams DESC;\n",
    "\"\"\".format(tuple(top_one_songs_pd[\"title\"].to_list())))\n",
    "\n",
    "top_rank_one_songs_pd = best_of_top_one_songs.toPandas()\n",
    "with open(results_path + \"top_rank_one_songs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(top_rank_one_songs_pd, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rank ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----------+--------------------+--------------------+---------+------+-------------+-------+\n",
      "|               title|rank|      date|              artist|                 url|   region| chart|        trend|streams|\n",
      "+--------------------+----+----------+--------------------+--------------------+---------+------+-------------+-------+\n",
      "|Chantaje (feat. M...|   1|2017-01-01|             Shakira|https://open.spot...|Argentina|top200|SAME_POSITION| 253019|\n",
      "|Vente Pa' Ca (fea...|   2|2017-01-01|        Ricky Martin|https://open.spot...|Argentina|top200|      MOVE_UP| 223988|\n",
      "|Reggaet√≥n Lento (...|   3|2017-01-01|                CNCO|https://open.spot...|Argentina|top200|    MOVE_DOWN| 210943|\n",
      "|              Safari|   4|2017-01-01|J Balvin, Pharrel...|https://open.spot...|Argentina|top200|SAME_POSITION| 173865|\n",
      "|         Shaky Shaky|   5|2017-01-01|        Daddy Yankee|https://open.spot...|Argentina|top200|      MOVE_UP| 153956|\n",
      "|         Traicionera|   6|2017-01-01|     Sebastian Yatra|https://open.spot...|Argentina|top200|    MOVE_DOWN| 151140|\n",
      "|Cuando Se Pone a ...|   7|2017-01-01|              Rombai|https://open.spot...|Argentina|top200|    MOVE_DOWN| 148369|\n",
      "|Otra vez (feat. J...|   8|2017-01-01|       Zion & Lennox|https://open.spot...|Argentina|top200|    MOVE_DOWN| 143004|\n",
      "|        La Bicicleta|   9|2017-01-01|Carlos Vives, Sha...|https://open.spot...|Argentina|top200|      MOVE_UP| 126389|\n",
      "|Dile Que Tu Me Qu...|  10|2017-01-01|               Ozuna|https://open.spot...|Argentina|top200|    MOVE_DOWN| 112012|\n",
      "|  Andas En Mi Cabeza|  11|2017-01-01|Chino & Nacho, Da...|https://open.spot...|Argentina|top200|SAME_POSITION| 110395|\n",
      "|Desde Esa Noche (...|  12|2017-01-01|              Thalia|https://open.spot...|Argentina|top200|      MOVE_UP| 104592|\n",
      "|      Borro Cassette|  13|2017-01-01|              Maluma|https://open.spot...|Argentina|top200|      MOVE_UP| 101535|\n",
      "|Gyal You A Party ...|  14|2017-01-01|Charly Black, Dad...|https://open.spot...|Argentina|top200|    MOVE_DOWN|  99722|\n",
      "|Me llamas (feat. ...|  15|2017-01-01|             Piso 21|https://open.spot...|Argentina|top200|    MOVE_DOWN|  95010|\n",
      "|La Bicicleta (fea...|  16|2017-01-01|Carlos Vives, Sha...|https://open.spot...|Argentina|top200|      MOVE_UP|  92723|\n",
      "|DUELE EL CORAZON ...|  17|2017-01-01|    Enrique Iglesias|https://open.spot...|Argentina|top200|      MOVE_UP|  91325|\n",
      "|     Let Me Love You|  18|2017-01-01|DJ Snake, Justin ...|https://open.spot...|Argentina|top200|    MOVE_DOWN|  87926|\n",
      "|La Noche No Es pa...|  19|2017-01-01|         Mano Arriba|https://open.spot...|Argentina|top200|SAME_POSITION|  87033|\n",
      "|          Vacaciones|  20|2017-01-01|               Wisin|https://open.spot...|Argentina|top200|    MOVE_DOWN|  86103|\n",
      "+--------------------+----+----------+--------------------+--------------------+---------+------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_path + \"best_song.pkl\", \"rb\") as f:\n",
    "    best_song_pd = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:====================================================>    (24 + 2) / 26]\r"
     ]
    }
   ],
   "source": [
    "sonts_to_model = spark.sql(\"\"\"\n",
    "SELECT title, artist, rank, streams, date, region\n",
    "FROM charts\n",
    "WHERE title in {}\n",
    "\"\"\".format(tuple(best_song_pd[:10][\"title\"].to_list())))\n",
    "sonts_to_model_pd = sonts_to_model.toPandas()\n",
    "sonts_to_model_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, avg, col, format_number\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, SparkSession, Row\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "indexers = [StringIndexer(inputCol=c, outputCol='idx_' + c) for c in ['title', 'artist', 'region']]\n",
    "encoders = [OneHotEncoder(inputCol='idx_' + c, outputCol='ohe_' + c) for c in ['title', 'artist', 'region']]\n",
    "\n",
    "assembler_before_scaling = VectorAssembler(\n",
    "    inputCols=['ohe_title', 'ohe_artist', 'ohe_region'],\n",
    "    outputCol='assembled_features'\n",
    ")\n",
    "\n",
    "streams_assembler = VectorAssembler(inputCols=['streams'], outputCol='vec_streams')\n",
    "rank_assembler = VectorAssembler(inputCols=['rank'], outputCol='vec_rank')\n",
    "\n",
    "streams_scaler = MinMaxScaler(inputCol='vec_streams', outputCol='scaled_streams')\n",
    "rank_scaler = MinMaxScaler(inputCol='vec_rank', outputCol='scaled_rank')\n",
    "\n",
    "assembler_final = VectorAssembler(\n",
    "    inputCols=['assembled_features', 'scaled_streams'],\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "all_stages = indexers + encoders + [assembler_before_scaling, streams_assembler, rank_assembler, streams_scaler, rank_scaler, assembler_final]\n",
    "\n",
    "pipeline = Pipeline(stages=all_stages)\n",
    "df_new = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# def extract_first_element(v):\n",
    "#     return float(v[0])\n",
    "\n",
    "extract_first_element_udf = udf(lambda x: float(x[0]), DoubleType())\n",
    "select_df = df_new.withColumn('label', extract_first_element_udf(col('scaled_rank')))\n",
    "select_df = select_df.select(['features', 'label'])\n",
    "train_data, test_data = select_df.randomSplit([0.8, 0.2], seed=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/25 01:18:02 WARN DAGScheduler: Broadcasting large task binary with size 25.4 MiB\n",
      "ERROR:root:KeyboardInterrupt while sending command.                (0 + 4) / 26]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/olek/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/olek/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lr \u001b[38;5;241m=\u001b[39m LinearRegression(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m predictions \u001b[38;5;241m=\u001b[39m lr_model\u001b[38;5;241m.\u001b[39mtransform(test_data)\n\u001b[1;32m      4\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m RegressionEvaluator(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, predictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m, metricName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol='features', labelCol='label')\n",
    "lr_model = lr.fit(train_data)\n",
    "predictions = lr_model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "predictions.select('features', 'label', 'prediction').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
